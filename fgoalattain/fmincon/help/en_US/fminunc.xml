<?xml version="1.0" encoding="UTF-8"?>

<!--
 *
 * This help file was generated from fminunc.sci using help_from_sci().
 *
 -->

<refentry version="5.0-subset Scilab" xml:id="fminunc" xml:lang="en"
          xmlns="http://docbook.org/ns/docbook"
          xmlns:xlink="http://www.w3.org/1999/xlink"
          xmlns:svg="http://www.w3.org/2000/svg"
          xmlns:ns3="http://www.w3.org/1999/xhtml"
          xmlns:mml="http://www.w3.org/1998/Math/MathML"
          xmlns:scilab="http://www.scilab.org"
          xmlns:db="http://docbook.org/ns/docbook">

  <refnamediv>
    <refname>fminunc</refname>
    <refpurpose>Solves a Unconstrainted Optimization Problem</refpurpose>
  </refnamediv>


<refsynopsisdiv>
   <title>Calling Sequence</title>
   <synopsis>
   xopt = fminunc(f,x0)
   xopt = fminunc(f,x0,options)
   xopt = fminunc(f,x0,options,fGrad)
   xopt = fminunc(f,x0,options,fHess)
   xopt = fminunc(f,x0,options,fGrad,fHess)
   [xopt,fopt] = fminunc(.....)
   [xopt,fopt,exitflag]= fminunc(.....)
   [xopt,fopt,exitflag,output]= fminunc(.....)
   [xopt,fopt,exitflag,output,gradient]=fminunc(.....)
   [xopt,fopt,exitflag,output,gradient,hessian]=fminunc(.....)
   
   </synopsis>
</refsynopsisdiv>

<refsection>
   <title>Parameters</title>
   <variablelist>
   <varlistentry><term>f :</term>
      <listitem><para> a function, representing objective function of the problem</para></listitem></varlistentry>
   <varlistentry><term>x0 :</term>
      <listitem><para> a vector of doubles, containing starting of variables.</para></listitem></varlistentry>
   <varlistentry><term>options:</term>
      <listitem><para> a list, containing option for user to specify -Maximum iteration, Maximum CPU-time, Gradient&amp;  Hessian</para></listitem></varlistentry>
   <varlistentry><term>fGrad :</term>
      <listitem><para> a function, representing gradient function of the problem in Vector Form</para></listitem></varlistentry>
   <varlistentry><term>fHess :</term>
      <listitem><para> a function, representing hessian function of the problem in Symmetric Matrix form</para></listitem></varlistentry>
   <varlistentry><term>xopt :</term>
      <listitem><para> a vector of doubles, the computed solution of the optimization problem.</para></listitem></varlistentry>
   <varlistentry><term>fopt :</term>
      <listitem><para> a scalar of double, the function value at x.</para></listitem></varlistentry>
   <varlistentry><term>exitflag :</term>
      <listitem><para> a scalar of integer, containing flag which denotes the reason for termination of algorithm</para></listitem></varlistentry>
   <varlistentry><term>output   :</term>
      <listitem><para> a structure, containing information about the optimization.</para></listitem></varlistentry>
   <varlistentry><term>gradient :</term>
      <listitem><para> a vector of doubles, containing the gradient of the optimized point.</para></listitem></varlistentry>
   <varlistentry><term>hessian  :</term>
      <listitem><para> a matrix of doubles, containing the hessian of the optimized point.</para></listitem></varlistentry>
   </variablelist>
</refsection>

<refsection>
   <title>Description</title>
   <para>
Search the minimum of a unconstrained optimization problem specified by :
find the minimum of f(x) such that
   </para>
   <para>
<latex>
\begin{eqnarray}
&amp;\mbox{min}_{x}
&amp; f(x)\\
\end{eqnarray}
</latex>
   </para>
   <para>
We are calling IPOpt for solving the unconstrained problem, IPOpt is a library written in C++.
   </para>
   <para>
</para>
</refsection>

<refsection>
   <title>Examples</title>
   <programlisting role="example"><![CDATA[

//Find x in R^2 such that it minimizes rosenbrock function
//f = 100*(x2 - x1^2)^2 + (1-x1)^2

//Objective function to be minimised
function y= f(x)
y= 100*(x(2) - x(1)^2)^2 + (1-x(1))^2;
endfunction

//Starting point
x0=[-1,2];

//Options
options=list("MaxIter", [1500], "CpuTime", [500], "Gradient", "ON", "Hessian", "ON");

//Gradient of objective function
function y= fGrad(x)
y= [-400*x(1)*x(2) + 400*x(1)^3 + 2*x(1)-2, 200*(x(2)-x(1)^2)];
endfunction

//Hessian of Objective Function
function y= fHess(x)
y= [1200*x(1)^2- 400*x(2) + 2, -400*x(1);-400*x(1), 200 ];
endfunction

//Calling the Ipopt
[xopt,fopt,exitflag,output,gradient,hessian]=fminunc(f,x0,options,fGrad,fHess)


   ]]></programlisting>
</refsection>

<refsection>
   <title>Examples</title>
   <programlisting role="example"><![CDATA[

//Find x in R^2 such that the below function is minimum
//f = x1^2 + x2^2

//Objective function to be minimised
function y= f(x)
y= x(1)^2 + x(2)^2;
endfunction

//Starting point
x0=[2,1];

//Calling the Ipopt
[xopt,fopt]=fminunc(f,x0)

   ]]></programlisting>
</refsection>

<refsection>
   <title>Examples</title>
   <programlisting role="example"><![CDATA[

The below Problem is an Unbounded problem:
//Find x in R^2 such that the below function is minimum
//f = - x1^2 - x2^2

//Objective function to be minimised
function y= f(x)
y= -x(1)^2 - x(2)^2;
endfunction

//Starting point
x0=[2,1];
//Options
options=list("MaxIter", [1500], "CpuTime", [500], "Gradient", "ON", "Hessian", "ON");

//Gradient of objective function
function y= fGrad(x)
y= [-2*x(1),-2*x(2)];
endfunction

//Hessian of Objective Function
function y= fHess(x)
y= [-2,0;0,-2];
endfunction

//Calling the Ipopt
[xopt,fopt,exitflag,output,gradient,hessian]=fminunc(f,x0,options,fGrad,fHess)

   ]]></programlisting>
</refsection>

<refsection>
   <title>Authors</title>
   <simplelist type="vert">
   <member>R.Vidyadhar , Vignesh Kannan</member>
   </simplelist>
</refsection>
</refentry>
